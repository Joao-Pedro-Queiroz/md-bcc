{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406a75d6",
   "metadata": {},
   "source": [
    "# AF MEGADADOS 25-2\n",
    "**NOME**: SEU NOME AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bfd1f",
   "metadata": {},
   "source": [
    "## Permissões de uso\n",
    "\n",
    "Proibido publicar ou compartilhar este material com terceiros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b967ec6",
   "metadata": {},
   "source": [
    "## Regras\n",
    "- Esta prova é formada por 5 (cinco) questões e tem **duração de 2h00**.\n",
    "- Os exercícios com correção automática não terão correção manual, mas poderá haver checks de qualidade/honestidade. Caso tente burlar os testes, sua nota de prova será zero e será enviado ao regime disciplinar. O que é burlar? Por exemplo, para a pergunta \"calcule a média da coluna x\", o correto é fazer \"SELECT AVG(x) FROM tabela\", já fazer \"SELECT 100.10\" é considerado burlar os testes, pois sua query não funciona de forma genérica, apenas cola o próprio valor de resultado.\n",
    "- Você pode trazer uma folha de papel (tamanho próximo a A4 / oficio) preenchida dos dois lados e utilizar como consulta. A folha pode ser tanto impressa quanto escrita a mão, mas precisa ser física (não pode acessar no computador). Não é permitido utilizar lupa rsrs.\n",
    "- Será permitido utilizar o Workbench para testar queries e fazer forward / reverse engineering. Feche as abas com respostas realizadas durante o semestre.\n",
    "- Não é permitido compartilhar qualquer material durante a prova.\n",
    "- Não serão permitidas consultas a outros meios (material blackboard, sites, notebooks das aulas, github) impressos ou digitais, nem contactar outras pessoas. Caso ocorra, a nota da prova será zero e será enviado ao regime disciplinar.\n",
    "- Não será permitido o uso de ferramentas de IA (copilot, chatgpt e afins). Você precisará providenciar um ambiente seguro para a prova, sem extensões instaladas (não basta desativar) nem sites abertos. Caso ocorra, a nota da prova será zero e será enviado ao regime disciplinar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c0622",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "Não se esqueça de **entregar** o notebook ao final da prova. É obrigatório deixar as células **executadas**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9149680",
   "metadata": {},
   "source": [
    "## Insper autograding!\n",
    "\n",
    "Para receber feedback dos exercícios, iremos utilizar o `insper autograding`.\n",
    "\n",
    "**Importante**: você precisará do `.env`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7ad928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/macielcalebe/insperautograding.git\n",
      "  Cloning https://github.com/macielcalebe/insperautograding.git to /tmp/pip-req-build-i7bglcso\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/macielcalebe/insperautograding.git /tmp/pip-req-build-i7bglcso\n",
      "  Resolved https://github.com/macielcalebe/insperautograding.git to commit 49f1176548d7560b38d0314e1115f93467141893\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-dotenv (from insperautograder==0.4.0)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from insperautograder==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from insperautograder==0.4.0) (8.16.1)\n",
      "Collecting ipywidgets<=7.8.5 (from insperautograder==0.4.0)\n",
      "  Downloading ipywidgets-7.8.5-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<=7.8.5->insperautograder==0.4.0) (0.1.4)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<=7.8.5->insperautograder==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<=7.8.5->insperautograder==0.4.0) (5.11.2)\n",
      "Collecting widgetsnbextension~=3.6.10 (from ipywidgets<=7.8.5->insperautograder==0.4.0)\n",
      "  Downloading widgetsnbextension-3.6.10-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<=7.8.5->insperautograder==0.4.0)\n",
      "  Downloading jupyterlab_widgets-1.1.11-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->insperautograder==0.4.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->insperautograder==0.4.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->insperautograder==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->insperautograder==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->insperautograder==0.4.0) (2023.7.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->insperautograder==0.4.0) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->insperautograder==0.4.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->insperautograder==0.4.0) (0.2.8)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.11/site-packages (from widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (7.0.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->insperautograder==0.4.0) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->insperautograder==0.4.0) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->insperautograder==0.4.0) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython->insperautograder==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.8.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.25.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (4.0.7)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (6.3.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (4.0.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (23.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (8.4.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (5.4.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (7.9.2)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (5.9.2)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (7.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (23.2)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.17.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (25.1.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.6.4)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (6.25.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.13.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (4.19.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (3.11.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.2.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.18.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.8.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.5.8)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (5.9.5)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.4)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.10->ipywidgets<=7.8.5->insperautograder==0.4.0) (2.8.19.14)\n",
      "Downloading ipywidgets-7.8.5-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading jupyterlab_widgets-1.1.11-py3-none-any.whl (246 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-3.6.10-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: insperautograder\n",
      "  Building wheel for insperautograder (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for insperautograder: filename=insperautograder-0.4.0-py3-none-any.whl size=4568 sha256=9f7396058bccae5e85fb5d167d4ed25ab18c492a093682b147d207efc52b6b06\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5mtyd7fp/wheels/bb/01/cc/7f397587bfedd6c633e2627496c45d5693aa959ee8237ead1f\n",
      "Successfully built insperautograder\n",
      "Installing collected packages: python-dotenv, jupyterlab-widgets, widgetsnbextension, ipywidgets, insperautograder\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.9\n",
      "    Uninstalling jupyterlab-widgets-3.0.9:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.9\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.9\n",
      "    Uninstalling widgetsnbextension-4.0.9:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.9\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.1.1\n",
      "    Uninstalling ipywidgets-8.1.1:\n",
      "      Successfully uninstalled ipywidgets-8.1.1\n",
      "Successfully installed insperautograder-0.4.0 ipywidgets-7.8.5 jupyterlab-widgets-1.1.11 python-dotenv-1.2.1 widgetsnbextension-3.6.10\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/macielcalebe/insperautograding.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8f36b",
   "metadata": {},
   "source": [
    "## Como resolver os exercícios?\n",
    "\n",
    "No exercício 1, rode o Docker para resolver as questões de programação funcional.\n",
    "\n",
    "Nas demais, crie a base da prova em sua máquina. Utilize o notebook, MySQL Workbench ou o conector para testar as queries e soluções. Quando estiver bastante certo de que a resposta está correta, faça a submissão para o servidor.\n",
    "Para macOS e linux, utilize:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v \"`pwd`\":/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Se estiver no Windows estes comandos, utilize:\n",
    "\n",
    "- No Powershell: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v ${PWD}:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "- No Prompt de comando: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v %cd%:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "Agora abra esse notebook lá no container!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7434c",
   "metadata": {},
   "source": [
    "## Import das bibliotecas\n",
    "\n",
    "Vamos realizar o import das bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a15129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import insperautograder.jupyter as ia\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64245fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AF_MD_25_2\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f21c3e",
   "metadata": {},
   "source": [
    "### Notas\n",
    "\n",
    "A prova vale 10,5 pontos:\n",
    "\n",
    "- As primeiras três questões, que possuem correção automática, valem 7 pontos.\n",
    "- A questão 4 vale 3 pontos.\n",
    "- A questão 5 é de nota extra (0,5 pontos).\n",
    "\n",
    "Na API de correção automática a nota de cada questão será ponderada pelo seu peso. A nota será apresentada no intervalo 0 a 10, multiplique por 0.70 para saber a nota final considerando toda a prova.\n",
    "\n",
    "Para conferir a nota da correção automática da prova, utilize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d423a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade   | Exercício   |   Peso |   Nota |   Nota Sem Atraso |   Nota Com Atraso |\n",
       "|---:|:------------|:------------|-------:|-------:|------------------:|------------------:|\n",
       "|  0 | af_md_25_2  | ex01a       |      1 |      0 |                 0 |                 0 |\n",
       "|  1 | af_md_25_2  | ex01b       |      1 |      0 |                 0 |                 0 |\n",
       "|  2 | af_md_25_2  | ex01c       |      1 |      0 |                 0 |                 0 |\n",
       "|  3 | af_md_25_2  | ex02        |      1 |      0 |                 0 |                 0 |\n",
       "|  4 | af_md_25_2  | ex03        |      1 |      0 |                 0 |                 0 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.grades(task=\"af_md_25_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c25125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Tarefa     |   Nota | Conta como ATV?   |\n",
       "|---:|:-----------|-------:|:------------------|\n",
       "|  0 | af_md_25_2 |      0 | Não               |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.grades(by=\"TASK\", task=\"af_md_25_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e8e29",
   "metadata": {},
   "source": [
    "**Exercício 1 - Analytics de Podcasts**\n",
    "\n",
    "<img src=\"img/podcast_1.jpeg\">\n",
    "\n",
    "A partir dos dados transacionais da plataforma Xtremo (descrita no exercício 2), foi criado um arquivo para análise dos podcasts.\n",
    "\n",
    "O formato de cada linha do arquivo é:\n",
    "\n",
    "```python\n",
    "\"<ID_PODCAST>,<NOME_PODCAST>,<STATUS>,<NOME_HOST_LIDER>,<NOME_HOST_2>,...,<NOME_HOST_N>\"\n",
    "```\n",
    "\n",
    "Onde:\n",
    "\n",
    "- `<ID_PODCAST>`: número inteiro que identifica o podcast.\n",
    "- `<NOME_PODCAST>`: nome do podcast.\n",
    "- `<STATUS>`: status do podcast. Por exemplo: Ativo, Inativo. Outros status podem existir.\n",
    "- `<NOME_HOST_LIDER>`: nome do host líder do podcast.\n",
    "- `<NOME_HOST_2>`: nome do segundo host do podcast (se houver).\n",
    "- ...\n",
    "- `<NOME_HOST_N>`: nome do enésimo host do podcast (se houver).\n",
    "\n",
    "**Dica:** repare bem o padrão de separadores seguido por todas as linhas (vírgula `,`)!\n",
    "\n",
    "Você encontrará exemplos nos arquivos `data/podcasts_*.csv`. Utilize estes arquivos como base para os exercícios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b667ca",
   "metadata": {},
   "source": [
    "**a)** Crie uma função `podcast_com_mais_hosts` que recebe (nesta ordem):\n",
    "\n",
    "- o RDD no formato acima.\n",
    "\n",
    "E retorna uma tupla `(nome_podcast, k)` com o nome do podcast com maior número de membros (hosts), seguido da quantidade de hosts.\n",
    "\n",
    "**Obs**: Não que faça diferença neste exercício, mas sempre que falarmos dos nomes dos hosts, considere que o host líder também conta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf642939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('All Pacas', 6)\n"
     ]
    }
   ],
   "source": [
    "def podcast_com_mais_hosts(rdd):\n",
    "    podcasts = rdd.map(lambda linha: (linha.split(\",\")[1], len(linha.split(\",\")[2:])))\n",
    "    maior_podcast = podcasts.max(lambda x: x[1])\n",
    "    return maior_podcast\n",
    "\n",
    "\n",
    "rdd = sc.textFile(\"data/podcasts_1.csv\")\n",
    "\n",
    "# Descomente caso queira executar localmente\n",
    "print(podcast_com_mais_hosts(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a0234",
   "metadata": {},
   "source": [
    "Um teste local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "# # Garantir uso do RDD correto\n",
    "# rdd = sc.textFile(\"data/podcasts_2.csv\")\n",
    "# assert podcast_com_mais_hosts(rdd) == ('Mestres do Codigo', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381aed1",
   "metadata": {},
   "source": [
    "Após testar localmente e considerar sua solução correta, faça o envio clicando no botão abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1df6546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9360fe373c48508fe3c2616e077d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex01a', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.sender(answer=\"podcast_com_mais_hosts\", task=\"af_md_25_2\", question=\"ex01a\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f08206",
   "metadata": {},
   "source": [
    "**b)** Crie uma função `freq_absoluta_status(rdd, status)` que recebe um RDD no formato descrito e uma string `status`.\n",
    "\n",
    "Retorne quantos podcasts com o status fornecido existem (ou seja, a frequência absoluta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2cf3c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def freq_absoluta_status(rdd, status):\n",
    "    podcasts = rdd.filter(lambda linha: linha.split(\",\")[2] == status)\n",
    "    return podcasts.count()\n",
    "\n",
    "rdd = sc.textFile(\"data/podcasts_1.csv\")\n",
    "\n",
    "# Descomente caso queira executar localmente\n",
    "print(freq_absoluta_status(rdd, \"Active\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c45df",
   "metadata": {},
   "source": [
    "Alguns testes locais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "\n",
    "# # Garantir uso do RDD 2\n",
    "# rdd2 = sc.textFile(\"data/podcasts_2.csv\")\n",
    "\n",
    "# assert freq_absoluta_status(rdd2, \"Ativo\") == 7\n",
    "# assert freq_absoluta_status(rdd2, \"Inativo\") == 3\n",
    "\n",
    "# # Garantir uso do RDD 6\n",
    "# rdd6 = sc.textFile(\"data/podcasts_6.csv\")\n",
    "\n",
    "# assert freq_absoluta_status(rdd6, \"No ar\") == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378faed",
   "metadata": {},
   "source": [
    "Após testar localmente e considerar sua solução correta, faça o envio clicando no botão abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.sender(answer=\"freq_absoluta_status\", task=\"af_md_25_2\", question=\"ex01b\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe17777",
   "metadata": {},
   "source": [
    "**c)** Crie uma função `freq_nomes(rdd, status, k)` que recebe o RDD no formato descrito, um `status` (string) e um inteiro `k`.\n",
    "\n",
    "Considerando apenas os podcasts com o **status** definido, a função deve calcular a **frequência relativa** de cada **nome de host** e retornar uma lista de tuplas no formato `(nome_host, frequencia)`, ordenadas de forma decrescente pela frequência. O parâmetro `k` deve ser utilizado para limitar o número de resultados retornados.\n",
    "\n",
    "**Obs**:\n",
    "\n",
    "- Compare os nomes de forma *case-sensitive* (maiúsculas e minúsculas não são equivalentes).\n",
    "- A frequência relativa deverá ser representada como um valor percentual inteiro entre **0 e 100**, arredondado para cima com `math.ceil`.\n",
    "- O servidor de correção já tem a biblioteca `math` importada.\n",
    "\n",
    "**Atenção**: Esta questão pode ser mais complexa. Recomendamos que você a resolva por último!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e00a390d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 35) (18bf150175b9 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n           ^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\nTypeError: freq_nomes.<locals>.<lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n           ^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\nTypeError: freq_nomes.<locals>.<lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Descomente caso queira executar localmente\u001b[39;00m\n\u001b[1;32m     12\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/podcasts_2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfreq_nomes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAtivo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m, in \u001b[0;36mfreq_nomes\u001b[0;34m(rdd, status, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m hosts_tuplas \u001b[38;5;241m=\u001b[39m hosts_join\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m hosts_contagem \u001b[38;5;241m=\u001b[39m hosts_tuplas\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m w, a, b: (w, a \u001b[38;5;241m+\u001b[39m b))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhosts_contagem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 35) (18bf150175b9 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n           ^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\nTypeError: freq_nomes.<locals>.<lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n           ^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\nTypeError: freq_nomes.<locals>.<lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def freq_nomes(rdd, status, k):\n",
    "    podcasts = rdd.filter(lambda linha: linha.split(\",\")[2] == status)\n",
    "    hosts = podcasts.map(lambda linha: linha.split(\",\")[3:])\n",
    "    hosts_join = hosts.flatMap(lambda linha: linha)\n",
    "    hosts_tuplas = hosts_join.map(lambda x: (x, 1))\n",
    "    hosts_contagem = hosts_tuplas.reduceByKey(lambda w: (w, 0))\n",
    "    return hosts_contagem.take(k)\n",
    "\n",
    "# Descomente caso queira executar localmente\n",
    "rdd = sc.textFile(\"data/podcasts_2.csv\")\n",
    "print(freq_nomes(rdd, 'Ativo', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103db96",
   "metadata": {},
   "source": [
    "Alguns testes locais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "\n",
    "# rdd = sc.textFile(\"data/podcasts_2.csv\")\n",
    "\n",
    "# result = freq_nomes(rdd, \"Ativo\", 3)\n",
    "# assert all(isinstance(x[0], str) for x in result)\n",
    "# assert all(isinstance(x[1], int) for x in result)\n",
    "# assert result == [('Alice', 16), ('Igor', 12), ('Bob', 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fcff5",
   "metadata": {},
   "source": [
    "Após testar localmente e considerar sua solução correta, faça o envio clicando no botão abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d336f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.sender(answer=\"freq_nomes\", task=\"af_md_25_2\", question=\"ex01c\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57a30e",
   "metadata": {},
   "source": [
    "Agora que você terminou a parte de programação funcional da prova, salve o notebook e continue no VS Code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ccaa7",
   "metadata": {},
   "source": [
    "## Plataforma de Podcast XTREMO!\n",
    "\n",
    "No próximo exercício, iremos trabalhar com a base de dados **XTREMO**.\n",
    "\n",
    "O banco de dados `xtremo` é um sistema relacional sintético projetado para **gerenciar informações de um ecossistema de podcasts**. O foco é manter o registro de aspectos da produção, distribuição e monetização de conteúdo de áudio. Ele contém tabelas que cobrem as seguintes áreas principais:\n",
    "\n",
    "- **Conteúdo:** A tabela **`podcast`** armazena os dados principais dos programas, enquanto a tabela **`episodio`** detalha cada episódio.\n",
    "- **Participantes:** O banco gerencia informações sobre os **`hosts`** (apresentadores) e **`convidados`**, incluindo detalhes de contato e participação. A tabela **`podcast_host`** relaciona hosts a podcasts, e **`episodio_convidado`** liga convidados a episódios.\n",
    "- **Monetização:** As tabelas **`anunciante`**, **`tipo_anuncio`**, e **`anuncio`** lidam com dados comerciais. A tabela **`anuncio_episodio`** associa anúncios a episódios específicos, registrando informações como valor pago e ouvintes no momento da veiculação.\n",
    "- **Distribuição e Engajamento:** As tabelas **`plataforma_distribuicao`** e **`episodio_plataforma`** controlam a presença dos episódios em diferentes plataformas. A tabela **`topico`** e **`episodio_topico`** permitem categorizar o conteúdo e as **`mensagens`** coletam dados de interação do público."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6ca1f",
   "metadata": {},
   "source": [
    "### Instalação da base\n",
    "\n",
    "Execute os scripts `xtremo_0001.sql` e `xtremo_0002.sql` no **MySQL Workbench**. Estes scripts criam um banco `xtremo` e inserem dados de exemplo para resolução da prova.\n",
    "\n",
    "O banco de dados pode ser representado pelo seguinte diagrama do modelo relacional (também disponível em PDF na pasta `docs`):\n",
    "\n",
    "<img src=\"img/xtremo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc4a7e",
   "metadata": {},
   "source": [
    "Vamos criar nosso HELPER de conexão com o banco!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from functools import partial\n",
    "import os\n",
    "import insperautograder.jupyter as ia\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76115a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "def get_connection_helper():\n",
    "\n",
    "    def run_db_query(connection, query, args=None):\n",
    "        with connection.cursor() as cursor:\n",
    "            print(\"Executando query:\")\n",
    "            for result in cursor.execute(query, multi=True):\n",
    "                if result.with_rows:\n",
    "                    for row in result.fetchall():\n",
    "                        print(row)\n",
    "                else:\n",
    "                    print(f\"{result.rowcount} linhas afetadas.\")\n",
    "\n",
    "    connection = mysql.connector.connect(\n",
    "        host=os.getenv(\"MD_DB_SERVER\"),\n",
    "        user=os.getenv(\"MD_DB_USERNAME\"),\n",
    "        password=os.getenv(\"MD_DB_PASSWORD\"),\n",
    "        database=\"xtremo\",\n",
    "    )\n",
    "    return connection, partial(run_db_query, connection)\n",
    "\n",
    "\n",
    "connection, db = get_connection_helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4926e",
   "metadata": {},
   "source": [
    "**Exercício 2 - Análise de Receita de Anúncios**:\n",
    "\n",
    "A equipe de analytics da Xtremo precisa frequentemente calcular a receita total gerada por anúncios de um determinado anunciante em um período específico.\n",
    "\n",
    "Crie uma função `receita_anunciante` no MySQL que recebe:\n",
    "- `p_id_anunciante`: ID do anunciante (INT)\n",
    "- `p_data_inicio`: data de início do período (DATE)\n",
    "- `p_data_fim`: data de fim do período (DATE)\n",
    "\n",
    "A função deve retornar o valor total pago (`valor_pago`) pelos anúncios deste anunciante que foram lidos completamente (`lido_completo = 1`) no período especificado (intervalo fechado).\n",
    "\n",
    "Retorne `DECIMAL(12,2)`. Se não houver anúncios no período, retorne `0.00`.\n",
    "\n",
    "**SQL de referência:**\n",
    "\n",
    "```sql\n",
    "DROP FUNCTION IF EXISTS calcula_algo;\n",
    "\n",
    "CREATE FUNCTION calcula_algo(p_id_pessoa INT)\n",
    "RETURNS DECIMAL(5,2)\n",
    "READS SQL DATA\n",
    "BEGIN\n",
    "    DECLARE valor DECIMAL(5,2);\n",
    "\n",
    "    RETURN 0.0;\n",
    "END;\n",
    "```\n",
    "\n",
    "**Dica**:\n",
    "\n",
    "- Considere a coluna `momento_leitura` da tabela `anuncio_episodio` para verificar se o anúncio está no período.\n",
    "- O valor a ser somado é o `valor_pago` da tabela `anuncio_episodio`.\n",
    "- Garanta que está retornando no formato (tipo de dados) esperado.\n",
    "- Utilize `DATE(x.algum_campo_datetime)` para extrair a parte da data (sem hora) de um datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a275039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ex02 = \"\"\"\n",
    "-- Seu SQL AQUI!\n",
    "\"\"\"\n",
    "\n",
    "# Descomente caso queira executar localmente\n",
    "# db(sql_ex02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17292e",
   "metadata": {},
   "source": [
    "Exemplo de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd03fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "# db(\"SELECT receita_anunciante(1, '2023-06-01', '2023-08-01')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc548269",
   "metadata": {},
   "source": [
    "Após testar localmente e considerar sua solução correta, faça o envio clicando no botão abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a82e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.sender(answer=\"sql_ex02\", task=\"af_md_25_2\", question=\"ex02\", answer_type=\"pyvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c7c0c",
   "metadata": {},
   "source": [
    "**Exercício 3 - Estimativa de Armazenamento de Áudio**:\n",
    "\n",
    "<img src=\"img/storage.jpeg\">\n",
    "\n",
    "A Xtremo precisa estimar quanto espaço de armazenamento será necessário para hospedar os episódios de podcasts em diferentes qualidades de áudio.\n",
    "\n",
    "Áudios podem ser armazenados em diferentes *bitrates* (taxa de bits por segundo, quantos bits são necessários para representar um segundo de áudio), que afetam tanto a qualidade quanto o tamanho do arquivo.\n",
    "\n",
    "Crie uma função `estimar_armazenamento_podcast(duracao_minutos, bitrate_kbps, num_episodios)` que calcula o espaço total necessário em GB para armazenar uma temporada de podcasts.\n",
    "\n",
    "**Parâmetros:**\n",
    "- `duracao_minutos`: duração média de cada episódio em minutos (int)\n",
    "- `bitrate_kbps`: taxa de bits em kilobits por segundo (int)\n",
    "- `num_episodios`: número total de episódios (int)\n",
    "\n",
    "\n",
    "**Obs:**\n",
    "- Retorne o valor em GB arredondado para cima usando `math.ceil(valor)`.\n",
    "- Considere: 1 byte = 8 bits, 1 KB = 1000 bytes, 1 MB = 1000 KB, 1 GB = 1000 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def estimar_armazenamento_podcast(duracao_minutos, bitrate_kbps, num_episodios):\n",
    "    tamanho_total_gb = 0\n",
    "\n",
    "    return math.ceil(tamanho_total_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76319b08",
   "metadata": {},
   "source": [
    "Exemplos de execução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "\n",
    "# print(estimar_armazenamento_podcast(60, 128, 100))  # 100 eps de 60min a 128kbps\n",
    "# print(estimar_armazenamento_podcast(30, 256, 50))   # 50 eps de 30min a 256kbps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea1f0a",
   "metadata": {},
   "source": [
    "Alguns testes locais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f184c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente caso queira executar localmente\n",
    "\n",
    "# assert estimar_armazenamento_podcast(60, 128, 100) == 6\n",
    "# assert estimar_armazenamento_podcast(30, 256, 50) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45097494",
   "metadata": {},
   "source": [
    "Após testar localmente e considerar sua solução correta, faça o envio clicando no botão abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd00095",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.sender(answer=\"estimar_armazenamento_podcast\", task=\"af_md_25_2\", question=\"ex03\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836cb17",
   "metadata": {},
   "source": [
    "**Exercício 4 - Sistema de Apoio a Criadores**: (**Nota: 3,0**)\n",
    "\n",
    "<img src=\"img/send_money.jpg\">\n",
    "\n",
    "A Xtremo quer implementar um sistema de financiamento coletivo (estilo Patreon ou Apoia.se) onde usuários podem apoiar financeiramente seus canais favoritos em troca de recompensas.\n",
    "\n",
    "Atualmente, os dados de apoios estão armazenados em uma única tabela denormalizada conforme mostrado abaixo:\n",
    "\n",
    "| id_apoio | data_inicio_apoio | id_usuario | nome_usuario | email_usuario | id_canal | nome_canal | tags_canal | id_nivel_recompensa | nome_nivel | valor_mensal | descricao_nivel | beneficios |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| 1 | 2024-01-10 | 501 | Lucas Mendes | lucas@email.com | 10 | Ciência Todo Dia | Ciência, Educação | 1 | Apoio Júnior | 15.00 | Acesso ao Discord | ['Badge Exclusivo', 'Chat Privado'] |\n",
    "| 2 | 2024-02-15 | 502 | Beatriz Lima | bia@email.com | 10 | Ciência Todo Dia | Ciência, Educação | 2 | Apoio Pleno | 30.00 | Vídeos Antecipados | ['Badge Exclusivo', 'Chat Privado', 'Vídeo Extra'] |\n",
    "| 3 | 2024-03-20 | 501 | Lucas Mendes | lucas@email.com | 20 | História Viva | História, Cultura | 5 | Mecenas | 50.00 | Nome nos Créditos | ['Livro Digital', 'Encontro Mensal'] |\n",
    "| 4 | 2024-04-05 | 503 | Fernando O. | fer@email.com | 20 | História Viva | História, Cultura | 5 | Mecenas | 50.00 | Nome nos Créditos | ['Livro Digital', 'Encontro Mensal'] |\n",
    "| 5 | 2024-04-10 | 502 | Beatriz Lima | bia@email.com | 30 | Tech News | Tecnologia | 8 | Insider | 20.00 | Newsletter Semanal | ['News VIP'] |\n",
    "\n",
    "**Descrição dos Campos:**\n",
    "\n",
    "- `id_apoio (PK)`: Identificador único do registro de apoio.\n",
    "- `data_inicio_apoio`: Data em que o usuário começou a apoiar.\n",
    "- `id_usuario`: Identificador do usuário apoiador. Um usuário pode apoiar diversos canais.\n",
    "- `nome_usuario`: Nome completo do usuário. Considere que não é necessário consultar nome e sobrenome separadamente.\n",
    "- `email_usuario`: Email do usuário.\n",
    "- `id_canal`: Identificador do canal/criador sendo apoiado.\n",
    "- `nome_canal`: Nome do canal sendo apoiado.\n",
    "- `tags_canal`: Tags ou categorias do canal sendo apoiado (múltiplas tags separadas por vírgula). Considere que é necessário consultar cada tag individualmente.\n",
    "- `id_nivel_recompensa`: Identificador do nível de apoio escolhido (tier). Cada apoio refere-se a um nível específico. Cada canal tem seus próprios níveis de apoio.\n",
    "- `nome_nivel`: Nome do nível de apoio (ex: \"Apoio Júnior\", \"Mecenas\").\n",
    "- `valor_mensal`: Valor monetário cobrado mensalmente neste nível. Todos que estejam em um mesmo nível pagam o mesmo valor.\n",
    "- `descricao_nivel`: Descrição textual geral das recompensas do nível.\n",
    "- `beneficios`: Lista de benefícios específicos incluídos neste nível de recompensa. Considere que é necessário consultar quais níveis oferecem um benefício específico. Considere que os benefícios são compartilhados entre os níveis.\n",
    "\n",
    "**Tarefa**:\n",
    "\n",
    "Normalize os dados até a 3ª Forma Normal (3NF), garantindo:\n",
    "\n",
    "1. Eliminação de redundâncias.\n",
    "1. Tratamento correto de atributos multivalorados.\n",
    "1. Integridade referencial entre as entidades.\n",
    "1. Relacionamentos corretos.\n",
    "\n",
    "Crie no **MySQL Workbench** o **Diagrama do Modelo Relacional (Diagrama de Entidades e Relacionamentos Extendido [DEER])** que representa a base de dados normalizada.\n",
    "\n",
    "**Instruções:**\n",
    "- Salve o DER na pasta `resposta_ex4`. Utilize formato **png** ou **jpg**.\n",
    "- Insira o caminho do arquivo no notebook.\n",
    "\n",
    "**Critérios de Avaliação (Rubrica):**\n",
    "\n",
    "| Conceito | Nota Porcentual | Descrição |\n",
    "|:----------:|----------:|:---------|\n",
    "| I | 0.0 | Insuficiente, não considera as informações a serem armazenadas ou cenário diferente do proposto |\n",
    "| D | 0.3 | Não está na 1NF, mas solução em desenvolvimento |\n",
    "| C | 0.6 | Está na 1NF, sem erros graves |\n",
    "| B | 0.8 | Está na 2NF, sem erros |\n",
    "| A | 1.0 | Está na 3NF, sem erros, redundâncias ou ineficiências |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb5435",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Seu diagrama do workbench AQUI!\n",
    "\n",
    "<img src=\"resposta_ex04/exemplo.png\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016e075",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Caso considere necessário, deixe AQUI uma explicação sobre as decisões que tomou neste exercício!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5db9fe",
   "metadata": {},
   "source": [
    "**Exercício 5)** (**Nota EXTRA: 0,5**) Seu colega (um mero mortal) realiza a seguinte pergunta:\n",
    "\n",
    "*\"Por que utilizar um SGBD relacional se posso salvar os dados do meu sistema gerencial em um CSV?\"*\n",
    "\n",
    "Responda de forma crítica. Justifique detalhadamente, de forma aprofundada.\n",
    "\n",
    "Esta questão será corrigida considerando a seguinte rubrica:\n",
    "\n",
    "| Conceito | Percentual da Nota | Descrição                                                                                                                                               |\n",
    "|:----------:|----------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| I        | 0.0 |Apenas citou o assunto ou alguns fatos sem explicações                                                                                                  |\n",
    "| D        | 0.3 |Explicou superficialmente o assunto ou fatos mas sem muitos detalhes conclusivos                                                                        |\n",
    "| C        | 0.6 |Explicou com detalhes, apresentando definições concretas.                                                                                               |\n",
    "| B        | 0.8 |Explicou com detalhes, apresentando definições concretas e exemplos de uso.                                                                            |\n",
    "| A        | 1.0 |Explicou com detalhes, apresentando definições concretas, exemplos de uso e ainda outros tópicos correlatos, fazendo uma conexão lógica entre eles. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9e385",
   "metadata": {},
   "source": [
    "## Referências das imagens\n",
    "\n",
    "Filtradas para licença Creative Commons no Google:\n",
    "\n",
    "- https://cdn.prod.website-files.com/637859b9a818d7ddb05bb5fb/64d06465a8410d89a3782469_637fc63ff88e999b5ddca8b0_Starting-a-Business-Podcast.jpeg\n",
    "- https://www.sectorlink.com/img/blog/server-ai.jpg\n",
    "- https://www.publicdomainpictures.net/pictures/270000/velka/money-transfer-mobile-banking-bu.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def97f9",
   "metadata": {},
   "source": [
    "### Conferindo as notas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747932f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.grades(task=\"af_md_25_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.grades(by=\"TASK\", task=\"af_md_25_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4742a",
   "metadata": {},
   "source": [
    "## Entrega!\n",
    "\n",
    "É hora de entregar. Faça **zip** (não RAR), envie no Blackboard e finalize o teste no proctorio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafad176",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Caso considere necessário, deixe AQUI comentários ou reclamações sobre qualquer questão da prova!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
